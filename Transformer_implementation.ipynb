{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RP-WxzjTeSDR",
        "outputId": "279a01e7-7a88-4bec-d845-c55f0a8b6017"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 1.2756e+00, -2.8853e-01, -1.6341e-02,  ..., -1.3297e+00,\n",
            "           3.1749e-01,  1.3849e-01],\n",
            "         [-9.1959e-01,  4.3836e-01, -6.2098e-01,  ..., -3.0692e-01,\n",
            "           1.2537e+00,  5.2725e-04],\n",
            "         [ 1.7660e-01,  2.3661e+00, -2.1267e-01,  ..., -1.5117e+00,\n",
            "          -6.5065e-01, -1.4829e-01],\n",
            "         [ 6.8536e-01,  8.3798e-02, -4.5726e-01,  ...,  7.4870e-01,\n",
            "           1.2463e+00,  1.0455e+00],\n",
            "         [-2.7685e-01,  1.2896e+00, -1.4539e+00,  ..., -1.2494e+00,\n",
            "          -1.1990e-01,  9.8863e-01]],\n",
            "\n",
            "        [[-1.2678e+00, -3.3537e-01,  1.0391e+00,  ...,  6.9800e-01,\n",
            "          -7.8852e-01, -5.4905e-01],\n",
            "         [ 1.0651e+00,  9.1889e-01,  1.5173e+00,  ...,  1.5635e+00,\n",
            "          -3.1303e-01,  6.6971e-04],\n",
            "         [-3.5550e-02, -3.0058e-01, -5.2690e-01,  ...,  4.9994e-01,\n",
            "           6.1160e-01,  5.0224e-01],\n",
            "         [-5.8075e-01,  6.5676e-01, -1.4737e-01,  ...,  2.6394e-01,\n",
            "           9.6445e-01, -5.8891e-01],\n",
            "         [ 1.6721e-01, -3.4282e-01, -1.0198e+00,  ...,  2.5008e-01,\n",
            "           7.8400e-01,  1.6432e+00]]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.embedding(x)\n",
        "\n",
        "# Example\n",
        "vocab_size = 10000  # Number of unique tokens\n",
        "embed_size = 512    # Size of each embedding vector\n",
        "embedding_layer = TokenEmbedding(vocab_size, embed_size)\n",
        "sample_input = torch.randint(0, vocab_size, (2, 5))  # Batch of 2, sequence length 5\n",
        "print(embedding_layer(sample_input))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, embed_size, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        positional_encoding = torch.zeros(max_len, embed_size)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, embed_size, 2).float() * (-math.log(10000.0) / embed_size))\n",
        "        positional_encoding[:, 0::2] = torch.sin(position * div_term)\n",
        "        positional_encoding[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.positional_encoding = positional_encoding.unsqueeze(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.positional_encoding[:, :x.size(1), :]\n",
        "\n",
        "# Example\n",
        "pos_encoding = PositionalEncoding(embed_size)\n",
        "sample_embed = torch.randn(2, 5, embed_size)  # Batch of 2, sequence length 5\n",
        "print(pos_encoding(sample_embed))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpETvO-be4XC",
        "outputId": "d336c558-a51b-474c-b174-3b95519009d5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-1.2256, -0.3302, -1.1003,  ...,  1.1093, -1.5659,  1.1703],\n",
            "         [ 0.7098,  0.3457,  0.2656,  ...,  1.7193,  0.3305,  1.4212],\n",
            "         [ 1.6793, -1.4432,  1.2259,  ...,  0.6950, -0.5887,  1.2472],\n",
            "         [ 0.5869, -1.2814,  0.8982,  ...,  1.9058,  0.3580, -0.2717],\n",
            "         [-1.5831, -1.2846, -0.5073,  ...,  1.3527,  0.7306,  1.6261]],\n",
            "\n",
            "        [[-0.3170,  2.2296,  0.2574,  ...,  2.4787, -0.2724,  0.6551],\n",
            "         [ 0.0692,  1.2288,  1.0194,  ...,  0.3034, -1.3004, -0.4429],\n",
            "         [ 0.1045, -0.5705,  0.9179,  ...,  1.5701, -1.6499,  1.2531],\n",
            "         [ 0.5472, -1.5761, -1.8515,  ...,  0.2490,  0.4323,  0.5445],\n",
            "         [-0.6237,  0.0706,  0.0886,  ...,  2.1376,  0.1846,  1.4615]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_size, heads):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.heads = heads\n",
        "        self.head_dim = embed_size // heads\n",
        "\n",
        "        assert self.head_dim * heads == embed_size, \"Embed size must be divisible by heads\"\n",
        "\n",
        "        self.values = nn.Linear(embed_size, embed_size)\n",
        "        self.keys = nn.Linear(embed_size, embed_size)\n",
        "        self.queries = nn.Linear(embed_size, embed_size)\n",
        "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
        "\n",
        "    def forward(self, values, keys, queries, mask):\n",
        "        N = queries.shape[0]  # Batch size\n",
        "        value_len, key_len, query_len = values.shape[1], keys.shape[1], queries.shape[1]\n",
        "\n",
        "        values = self.values(values)\n",
        "        keys = self.keys(keys)\n",
        "        queries = self.queries(queries)\n",
        "\n",
        "        # Split embeddings into multiple heads\n",
        "        values = values.view(N, value_len, self.heads, self.head_dim).transpose(1, 2)\n",
        "        keys = keys.view(N, key_len, self.heads, self.head_dim).transpose(1, 2)\n",
        "        queries = queries.view(N, query_len, self.heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])  # Dot product\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "\n",
        "        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n",
        "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(N, query_len, self.embed_size)\n",
        "        return self.fc_out(out)\n",
        "\n",
        "# Example\n",
        "self_attention = SelfAttention(embed_size, heads=8)\n",
        "sample_values = torch.randn(2, 5, embed_size)\n",
        "sample_keys = torch.randn(2, 5, embed_size)\n",
        "sample_queries = torch.randn(2, 5, embed_size)\n",
        "print(self_attention(sample_values, sample_keys, sample_queries, mask=None))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2_tnfMne-Gq",
        "outputId": "5a8eef2f-9f19-4b0e-b506-febe08dcaefa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.2051, -0.0942, -0.0996,  ..., -0.0270,  0.0003,  0.0634],\n",
            "         [-0.1861, -0.0685, -0.1287,  ...,  0.0260, -0.0940, -0.0952],\n",
            "         [ 0.0826,  0.0721, -0.0073,  ..., -0.0900, -0.1284, -0.0448],\n",
            "         [-0.0518,  0.1684,  0.1346,  ..., -0.1092,  0.0984,  0.0716],\n",
            "         [ 0.2453,  0.0283,  0.0512,  ...,  0.0241, -0.0437, -0.1226]],\n",
            "\n",
            "        [[ 0.0008,  0.0235, -0.1502,  ...,  0.0541,  0.0904,  0.1190],\n",
            "         [ 0.0168,  0.2034, -0.1096,  ...,  0.1638, -0.1925, -0.1107],\n",
            "         [ 0.0004,  0.0211, -0.1277,  ..., -0.2120,  0.0617, -0.0202],\n",
            "         [ 0.2216, -0.0952, -0.0638,  ...,  0.0396, -0.1255, -0.2428],\n",
            "         [-0.0822, -0.0051,  0.1309,  ...,  0.0268,  0.0958, -0.1995]]],\n",
            "       grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_dim):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.fc1 = nn.Linear(embed_size, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, embed_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(torch.relu(self.fc1(x)))\n",
        "\n",
        "# Example\n",
        "ffn = FeedForward(embed_size, hidden_dim=2048)\n",
        "sample_input = torch.randn(2, 5, embed_size)\n",
        "print(ffn(sample_input))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rL8UrIQgfDDD",
        "outputId": "cdbca1e2-e749-4881-c0c0-78b6952cc540"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.1399, -0.1748,  0.2533,  ...,  0.1207, -0.1825, -0.3562],\n",
            "         [ 0.0389, -0.1497,  0.1076,  ...,  0.0903, -0.1643, -0.1957],\n",
            "         [-0.3438, -0.5250,  0.3384,  ..., -0.0641,  0.0429, -0.4472],\n",
            "         [-0.1850, -0.1658,  0.0186,  ...,  0.2102,  0.1028, -0.2599],\n",
            "         [-0.3367,  0.1386,  0.2086,  ..., -0.0724,  0.2466, -0.6518]],\n",
            "\n",
            "        [[-0.0943, -0.1365,  0.1974,  ...,  0.2559, -0.0492, -0.1265],\n",
            "         [-0.3728,  0.0049,  0.2150,  ..., -0.0993,  0.1318,  0.0046],\n",
            "         [-0.1181, -0.2412, -0.1956,  ..., -0.0796, -0.0455, -0.1000],\n",
            "         [ 0.0208,  0.0765,  0.3855,  ...,  0.0871,  0.1456,  0.1169],\n",
            "         [-0.0728, -0.0939, -0.0690,  ...,  0.2313, -0.0984, -0.0998]]],\n",
            "       grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, embed_size, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones(embed_size))\n",
        "        self.beta = nn.Parameter(torch.zeros(embed_size))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        std = x.std(dim=-1, keepdim=True)\n",
        "        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
        "\n",
        "# Example\n",
        "layer_norm = LayerNorm(embed_size)\n",
        "sample_input = torch.randn(2, 5, embed_size)\n",
        "print(layer_norm(sample_input))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUkZ6MiWfGzs",
        "outputId": "bfb1b2ec-71b6-4e58-9762-914f9b7aca45"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-1.5051,  0.6230,  0.2745,  ...,  1.9000, -0.4936,  1.0943],\n",
            "         [-0.8244, -0.1761, -0.4022,  ...,  1.4115, -0.4973,  0.1845],\n",
            "         [-0.8476,  0.0574,  1.6289,  ...,  1.1036,  0.8037, -0.0115],\n",
            "         [ 1.4055, -2.2716, -0.7452,  ...,  2.5477, -0.0618,  0.9770],\n",
            "         [ 2.9755, -0.2280, -0.0464,  ..., -0.6853,  0.3125,  0.9810]],\n",
            "\n",
            "        [[-0.7845, -1.3037, -0.4398,  ..., -0.6197,  0.9313,  1.1225],\n",
            "         [-0.9677,  0.6140,  0.5200,  ..., -0.5276,  0.9966,  0.4008],\n",
            "         [ 0.7556,  1.0178, -1.4728,  ..., -1.1096,  0.9708,  0.3151],\n",
            "         [-0.7562,  1.7414, -1.3928,  ..., -0.2798, -0.0870, -1.3427],\n",
            "         [-2.6141, -2.7820, -0.4479,  ...,  0.4673,  0.9002, -0.4669]]],\n",
            "       grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_size, heads, hidden_dim, dropout):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.self_attention = SelfAttention(embed_size, heads)\n",
        "        self.norm1 = LayerNorm(embed_size)\n",
        "        self.norm2 = LayerNorm(embed_size)\n",
        "        self.feed_forward = FeedForward(embed_size, hidden_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, value, key, query, mask):\n",
        "        attention = self.self_attention(value, key, query, mask)\n",
        "        x = self.dropout(self.norm1(attention + query))\n",
        "        forward = self.feed_forward(x)\n",
        "        out = self.dropout(self.norm2(forward + x))\n",
        "        return out\n",
        "\n",
        "# Example\n",
        "transformer_block = TransformerBlock(embed_size, heads=8, hidden_dim=2048, dropout=0.1)\n",
        "sample_values = torch.randn(2, 5, embed_size)\n",
        "print(transformer_block(sample_values, sample_values, sample_values, mask=None))"
      ],
      "metadata": {
        "id": "SclYQSuofheV",
        "outputId": "5dcf0183-5d96-4396-a94a-716d16b6408c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-1.9844,  0.9456,  0.3492,  ..., -1.3219, -1.4275, -0.5619],\n",
            "         [ 1.3929, -0.6650, -2.1925,  ..., -0.3971,  0.1844, -0.1936],\n",
            "         [ 0.8262,  1.0770,  0.0754,  ...,  0.3134, -0.8512,  0.4292],\n",
            "         [-0.0914,  1.7182,  0.0000,  ..., -0.3007,  0.3823, -0.1301],\n",
            "         [ 0.1395,  0.0112, -0.1073,  ...,  0.4543,  0.7772, -0.3475]],\n",
            "\n",
            "        [[-0.8234,  0.4540, -1.1789,  ...,  1.5506,  1.1937, -0.8939],\n",
            "         [ 0.0000, -0.6291,  0.6088,  ...,  1.3344, -0.6786, -3.1985],\n",
            "         [-0.4257,  0.4937,  0.6166,  ..., -0.1715,  0.8155, -0.0806],\n",
            "         [-0.9355, -0.8333,  0.0000,  ...,  0.0000, -2.8921,  1.9766],\n",
            "         [ 0.1750,  0.5049,  2.5375,  ..., -0.6904, -0.2660,  1.2031]]],\n",
            "       grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Playing around with the Torch.nn module"
      ],
      "metadata": {
        "id": "Q7F3liEU6Yne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(3, 4)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7u2jd-VAJ8j",
        "outputId": "6cb65d03-9bfa-469a-9e2e-2371b1922ee1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-1.2724, -1.1546,  0.9854, -0.4189],\n",
            "        [ 0.2406, -0.0393,  0.1152,  0.4656],\n",
            "        [-0.2870,  0.7521,  0.0750,  0.0653]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer = nn.Linear(in_features=4, out_features=2)\n",
        "x = torch.randn(3, 4)  # batch size 3, input dim 4\n",
        "output = layer(x)\n",
        "print(\"Linear Output:\\n\", output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Azsssy0S6XhM",
        "outputId": "d5fc0116-3597-4c0b-d5b6-9a372e5b98f2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Output:\n",
            " tensor([[-0.6080, -0.3494],\n",
            "        [-0.6235, -0.5291],\n",
            "        [-0.2171,  0.2979]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "relu = nn.ReLU()\n",
        "sigmoid = nn.Sigmoid()\n",
        "\n",
        "x = torch.tensor([-1.0, 0.0, 1.0])\n",
        "print(\"ReLU:\", relu(x))\n",
        "print(\"Sigmoid:\", sigmoid(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeGzRqKUAU1q",
        "outputId": "6a47e762-04ac-40da-8976-d0145f2b4ecd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ReLU: tensor([0., 0., 1.])\n",
            "Sigmoid: tensor([0.2689, 0.5000, 0.7311])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.MSELoss()\n",
        "pred = torch.tensor([0.8, 0.4], requires_grad=True)\n",
        "target = torch.tensor([1.0, 0.0])\n",
        "loss = criterion(pred, target)\n",
        "print(\"MSE Loss:\", loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SM7T2LIbAYGo",
        "outputId": "09132ab3-e82c-4d7d-f52d-00f9238899af"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE Loss: 0.10000000149011612\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Linear(4, 8),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(8, 2)\n",
        ")\n",
        "\n",
        "x = torch.randn(1, 4)\n",
        "output = model(x)\n",
        "print(\"Sequential Model Output:\", output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0uqZrgZBVP4",
        "outputId": "2d81e423-af6f-4446-901a-c0b1f32bd74a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential Model Output: tensor([[ 0.3174, -0.5773]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(4, 8)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(8, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.relu(self.fc1(x)))\n",
        "\n",
        "net = MyNet()\n",
        "x = torch.randn(2, 4)\n",
        "print(\"Custom Net Output:\\n\", net(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pc6h6iaUBXtu",
        "outputId": "1302d215-495d-4187-a0ce-d8e9ea1f1f76"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom Net Output:\n",
            " tensor([[-0.1140,  0.3842],\n",
            "        [ 0.1333,  0.5911]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    }
  ]
}
